### **14.1.1. Основные принципы и применение искусственных нейронных сетей (ИНС)**

- **Компьютеры хороши в вычислениях**, но сложнее справляются с задачами, связанными с распознаванием образов (например, букв, форм или сигналов), классификацией объектов и ассоциациями.
- Искусственные нейронные сети помогают в этих сложных задачах. Их применяют в разных областях, таких как:
    - Проектирование космических кораблей.
    - Распознавание речи и текста.
    - Анализ и прогнозирование в финансах.
    - Информационная безопасность, компрессия изображений.
    - Криптография, например, для создания и согласования ключей.

**Пример:** Нейронные сети могут помочь создать надежные системы безопасности или сделать прогнозы на основе больших объемов данных.

---

### **14.1.2. Основы моделирования ИНС**

- **Нейроны в мозге:** Мозг состоит из клеток, нейронов, которые обрабатывают сигналы. Каждый нейрон имеет дендриты (принимают сигналы), тело клетки (суммирует их) и аксон (передает сигнал другим клеткам).
    
- **Искусственный нейрон (персептрон):** Создан на основе биологического нейрона. У него есть:
    
    - **Входы** – аналог дендритов.
    - **Веса** – аналог синапсов (определяют значимость сигнала).
    - **Суммирующий блок** – объединяет входные сигналы.
    - **Функция активации** – решает, передать ли сигнал дальше.
    - **Выход** – результат.

**Пример:** Если сигнал на входе достаточно сильный, нейрон "активируется" и передает сигнал дальше.

Сам нейрон называется персептронг, который симулирует работу биологического нейрона.

![[Pasted image 20241204002121.png]]

- **Обучение нейронов:** Чтобы сеть работала, ее нужно тренировать. Это процесс настройки "весов", чтобы сеть давала правильные результаты.
    - **Метод Хэбба:** Один из методов обучения, где веса корректируются на основе входных сигналов и результатов работы нейрона.

**Пример:** Нейронная сеть может научиться распознавать рукописные цифры после тренировки на наборе примеров.
![[Pasted image 20241204002248.png]]
---

### **14.1.3. Синхронизация нейронных сетей**

- **Идея:** Две нейронные сети могут учиться друг у друга, передавая сигналы и корректируя свои настройки. Это похоже на взаимодействие людей, которые договариваются о чем-то на основе взаимного обмена информацией.
- **Применение в криптографии:** Две сети могут согласовать общий секретный ключ, используя синхронизацию своих весов. Это может быть альтернативой известному протоколу Диффи-Хеллмана.

**Пример:** Вместо того, чтобы открыто передавать ключ, две сети учатся и приходят к одному и тому же результату без его раскрытия.

- **Tree Parity Machine (ТРМ):** Специальная структура для работы с битовыми данными (0 и 1). Она состоит из нескольких уровней нейронов, которые помогают синхронизировать ключи.

**Пример:** Эта модель позволяет безопасно обмениваться ключами в цифровых системах.
### **14.3. Архитектура Tree Parity Machine (ТРМ)**

На рисунке 14.3 показана структура двухуровневой модели ТРМ:

1. **Общая структура:**
    
    - Это сеть с двумя уровнями:
        - **I уровень**: Содержит несколько (K) персептронов.
        - **II уровень**: Объединяет выходы всех персептронов с помощью умножения.
2. **Персептроны:**
    
    - Каждый персептрон имеет N входов. Входные значения бинарные (либо -1, либо +1).
    - Веса wkj относятся к каждому входу xkj​ персептрона.
    - Выход yk​ вычисляется как сумма входов, умноженных на соответствующие веса, после чего применяется функция активации (например, знак числа).
3. **Выход всей архитектуры (O):**
    
    - Вычисляется как произведение выходов всех K персептронов:
	![[Pasted image 20241204123226.png]]
    - Это нестандартный подход для нейронных сетей: итоговый результат всей сети зависит от всех выходов скрытых персептронов.
4. **Веса:**
    
    - Веса каждого входа ограничены интервалом [−L,L], чтобы избежать бесконечного роста значений.

**Простое объяснение:**  
ТРМ — это сеть, состоящая из нескольких персептронов, объединённых так, что их результаты перемножаются для получения общего выхода. Эта структура используется для синхронизации (обучения) двух сетей, чтобы их веса в итоге стали одинаковыми.
![[Pasted image 20241204121608.png]]

---

### **14.4. Процесс синхронизации двух сетей**

На рисунке 14.4 показана схема синхронизации двух ТРМ:

1. **Сети A и B:**
    
    - Две сети (A и B) начинают с разными случайными весами.
    - На вход каждой сети подаётся один и тот же входной вектор X, состоящий из бинарных значений (-1 или +1).
2. **Вычисление выходов:**
    
    - Обе сети вычисляют свои выходы Oa и Ob​ по входному вектору X.
    - Если выходы совпадают (Oa = Ob​), то обе сети обновляют свои веса по специальному правилу.
3. **Обновление весов:**
    
    - Вес каждого персептрона обновляется только в том случае, если его выход совпадает с общим выходом сети. Это правило задаётся формулой (14.10).
    - Если веса выходят за пределы [−L,L], они корректируются (нормализуются).
4. **Секретный ключ:**
    
    - В процессе обучения сети постепенно синхронизируют свои веса (wA = wB​).
    - Эти веса используются как секретный ключ, поскольку они остаются скрытыми от третьих лиц.
5. **Противодействие атаке:**
    
    - Несмотря на то, что архитектура и передаваемые выходные значения известны, ключ (веса) остаётся секретным, так как третья сторона не знает начальных состояний весов.

**Простое объяснение:**  
Сети A и B обучаются на одинаковых входных данных, чтобы их веса в итоге совпали. Совпадение весов достигается через обмен выходами и последующее обновление весов. Полученные одинаковые веса становятся секретным ключом.
![[Pasted image 20241204002502.png]]